{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrate local data into openai knowledge base via vector-db\n",
    "## Part I: txt-files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector databases are a powerful and emerging class of databases engineered to manage and process structured data in a highly efficient way. They achieve this by indexing and storing vector embeddings, allowing for fast data retrieval. In this context, each data point is depicted as a numerical vector (embedding), making it well-suited for mathematical operations and analysis through machine learning algorithms.\n",
    "\n",
    "These databases empower vector-based search, also known as semantic search, not by relying on exact keyword matching, but by considering the actual meaning of the query. Through the encoding of datasets into meaningful vector representations, the distance between vectors reflects the similarities between the elements. Utilizing algorithms like Approximate Nearest Neighbor (ANN), they enable rapid retrieval of results that closely match the query, facilitating efficient and precise searches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![vector database](https://miro.medium.com/v2/resize:fit:640/format:webp/0*d8Utelp6ffNhi_eY.png)\n",
    "\n",
    "Source: https://odsc.medium.com/a-gentle-introduction-to-vector-search-3c0511bc6771"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import librarys and environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.llms import OpenAI\n",
    "#from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "import pickle\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API Reference:\n",
    "- [openai](https://platform.openai.com/docs/api-reference?lang=python)\n",
    "- [langchain document_loaders](https://python.langchain.com/docs/modules/data_connection/document_loaders/)\n",
    "- [langchain agents](https://python.langchain.com/docs/modules/agents/)\n",
    "- [FAISS vector database](https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.faiss.FAISS.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load documents, make embeddings and put them into a vector database\n",
    "- load documents into memory with <span style=\"color:#40E0D0\">langchain.document_loaders</span>\n",
    "- chunk documents into text pieces of given length and with given overlap to garantee for meaningful context with <span style=\"color:#40E0D0\">langchain.text_splitter.RecursiveCharacterTextSplitter</span>\n",
    "- create word embeddings in the form of embedding vectors for the given text with <span style=\"color:#40E0D0\">langchain.embeddings.OpenaiEmbeddings</span>\n",
    "- load the vectors into a vector database (FAISS) with <span style=\"color:#40E0D0\">langchain.vectorstores</span>\n",
    "- pickle the db for re-use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#document_loader = DirectoryLoader('./data', glob=\"**/*.txt\", loader_cls=TextLoader, show_progress=True)\n",
    "document_loader = TextLoader(\"./data/uncertainty.txt\")\n",
    "docs = document_loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Uncertainty', metadata={'source': './data/uncertainty.txt'})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "splitted_docs = text_splitter.split_documents(docs)\n",
    "splitted_docs[0] # show first document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(openai_api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb_for_txt = FAISS.from_documents(splitted_docs, embeddings)\n",
    "\n",
    "with open('vectordb_for_txt.pkl', 'wb') as file:\n",
    "    pickle.dump(vectordb_for_txt, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Query including vector database and query-history\n",
    "- load pickeled vectore database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('vectordb_for_txt.pkl', 'rb') as pickled_file:\n",
    "    vectorstore = pickle.load(pickled_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(\n",
    "    model_name=\"text-davinci-003\",\n",
    "    temperature=0,\n",
    "    openai_api_key=API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt_template = \"\"\"The following is a conversation with an AI research assistant. \n",
    "#The assistant tone is technical and scientific.The assistant answers should be easy to understand.\n",
    "#\"\"\"\n",
    "prompt_template = \"\"\"The following is a conversation with an AI research assistant. \n",
    "The assistant tone is technical and scientific.\n",
    "\n",
    "{context}\n",
    "\n",
    "User Question: {question}\n",
    "Answer AI Assistant: \"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=['context', 'question']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True, output_key='answer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    memory=chat_history,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    combine_docs_chain_kwargs={'prompt': PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Please give a short description (5 sentences) about the Markov model discribed in the retriever document.',\n",
       " 'chat_history': [HumanMessage(content='Please make a summary of a text about making optimal decisions given limited information and uncertainty given to you via retriever.', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='\\nThis lecture discusses how AI can make optimal decisions given limited information and uncertainty. An example is given of an AI trying to infer the weather, but only having access to an indoor camera that records how many people brought umbrellas with them. The lecture explains how AI can use a sensor model (also called an emission model) to represent the probabilities of the hidden state (the weather). The lecture also explains how AI can use a transition model to represent the probabilities of the hidden state changing over time. Finally, the lecture explains how AI can use a Bayesian filter to combine the sensor and transition models to make optimal decisions.', additional_kwargs={}, example=False),\n",
       "  HumanMessage(content='Please make summaries for the following topics: 1. Probability and Conditional Probability,     2. Random Variables, 3.Bayes Rule, 4. Joint Probability, 5. Probability Rules, 6. Bayesian Networks,     7. Sampling, 8. Markov Models and Hidden Markov Models based on the text given to you via retriever.', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='\\n\\nProbability: Probability is a measure of the likelihood of an event occurring. It is expressed as a number between 0 and 1, where 0 indicates that the event is impossible and 1 indicates that the event is certain.\\n\\nConditional Probability: Conditional probability is the probability of an event occurring given that another event has already occurred. It is expressed as a number between 0 and 1, where 0 indicates that the event is impossible and 1 indicates that the event is certain.\\n\\nRandom Variables: A random variable is a variable whose value is determined by a random process. It can take on any value within a given range.\\n\\nBayes Rule: Bayes Rule is a mathematical formula used to calculate the probability of an event occurring given the probability of another event occurring. It is expressed as P(A|B) = P(B|A) * P(A) / P(B).\\n\\nJoint Probability: Joint probability is the probability of two events occurring together. It is expressed as P(A,B) = P(A) * P(B|A).\\n\\nProbability Rules: Probability rules are mathematical formulas used to calculate the probability of an event occurring. Examples include Bayes', additional_kwargs={}, example=False),\n",
       "  HumanMessage(content='Please give a short description (5 sentences) about the Markov model discribed in the retriever document.', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='\\nThe Markov model described in the retriever document is a hidden Markov model. This type of model is used to represent a system with hidden states that generate some observed event. It is based on the idea that the AI has some measurement of the world but no access to the precise state of the world. The state of the world is called the hidden state and whatever data the AI has access to are the observations. The model is represented in a Markov chain with two layers, the top layer representing the hidden state and the bottom layer representing the evidence. This model can be used to predict future events based on the information of the one last event.', additional_kwargs={}, example=False)],\n",
       " 'answer': '\\nThe Markov model described in the retriever document is a hidden Markov model. This type of model is used to represent a system with hidden states that generate some observed event. It is based on the idea that the AI has some measurement of the world but no access to the precise state of the world. The state of the world is called the hidden state and whatever data the AI has access to are the observations. The model is represented in a Markov chain with two layers, the top layer representing the hidden state and the bottom layer representing the evidence. This model can be used to predict future events based on the information of the one last event.'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#qa({'question': 'How old is martin miller?'})\n",
    "qa({'question': 'Please give a short description (5 sentences) about the Markov model discribed in the retriever document.'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What does he love?',\n",
       " 'chat_history': [HumanMessage(content='How old is martin miller?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content=' Martin Miller is 60 years old.', additional_kwargs={}, example=False),\n",
       "  HumanMessage(content='What does he love?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content=' Martin Miller loves skiing, knitting, and writing good, concise, and clean Python code.', additional_kwargs={}, example=False)],\n",
       " 'answer': ' Martin Miller loves skiing, knitting, and writing good, concise, and clean Python code.'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa({'question': 'What does he love?'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- https://github.com/Coding-Crashkurse/LangChain-Basics/blob/main/basics.ipynb\n",
    "- https://python.langchain.com/docs/integrations/toolkits/document_comparison_toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
