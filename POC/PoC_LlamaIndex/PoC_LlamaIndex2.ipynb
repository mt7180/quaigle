{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat engine in Condense Question mode with explicitely specified vector_retriever and response_synthesizer\n",
    "### trying to integrate marvins ai_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from marvin import ai_model\n",
    "from llama_index.bridge.pydantic import BaseModel, Field\n",
    "from llama_index.callbacks import CallbackManager, TokenCountingHandler\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.llms import ChatMessage, MessageRole\n",
    "from llama_index import (\n",
    "    VectorStoreIndex, \n",
    "    SimpleDirectoryReader,\n",
    "    ServiceContext,\n",
    "    set_global_service_context,\n",
    "    get_response_synthesizer,\n",
    ")\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index.node_parser.extractors import (\n",
    "    MetadataExtractor,\n",
    ")\n",
    "from llama_index.text_splitter import TokenTextSplitter\n",
    "from llama_index.node_parser.extractors.marvin_metadata_extractor import (\n",
    "    MarvinMetadataExtractor,\n",
    ")\n",
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "\n",
    "from openai import log as openai_log\n",
    "import tiktoken\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import certifi\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "openai_log = \"debug\"\n",
    "\n",
    "load_dotenv()\n",
    "# load_dotenv(\"../.env\")\n",
    "# workaround for mac to solve \"SSL: CERTIFICATE_VERIFY_FAILED Error\"\n",
    "\n",
    "\n",
    "os.environ[\"REQUESTS_CA_BUNDLE\"] = certifi.where()\n",
    "os.environ[\"SSL_CERT_FILE\"] = certifi.where()\n",
    "\n",
    "API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "OPENAI_MODEL = \"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORY_LABELS = [\n",
    "    \"Technical\",\n",
    "    \"Science-Fiction\",\n",
    "    \"Poetry\",\n",
    "    \"Fantasy\",\n",
    "    \"Mystery\",\n",
    "    \"Romance\",\n",
    "    \"Historical\",\n",
    "    \"Fiction\",\n",
    "    \"Self-Help\",\n",
    "    \"Biography\",\n",
    "    \"Travelogue\",\n",
    "    \"Horror\",\n",
    "    \"Comedy\",\n",
    "    \"Thriller\",\n",
    "    \"Science\",\n",
    "    \"Philosophy\",\n",
    "    \"Memoir\",\n",
    "    \"Cookbook\",\n",
    "    \"Business\",\n",
    "    \"Drama\",\n",
    "    \"Satire\",\n",
    "]\n",
    "\n",
    "@ai_model\n",
    "class QueryDocument(BaseModel):\n",
    "    #name: str = Field(..., description=\"The name of the document\")\n",
    "    description: str = Field(..., description=\"a brief summary of the document content.\")\n",
    "    text_category: str = Field(...,description=f\"best matching text category from the following list: {str(CATEGORY_LABELS)}\")\n",
    "    \n",
    "    # def __init__(self, name):\n",
    "    #     super().__init__()\n",
    "    #     self.name = name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup:\n",
    "The Concepts Decoupling Chunks Used for Retrieval\" and \"Chunks Used for Synthesis\" are both used here.  \n",
    "\n",
    "The CondenseQuestionChatEngine is responsible for synthesis, while the RetrieverQueryEngine handles retrieval. The RetrieverQueryEngine uses a VectorIndexRetriever with a VectorStoreIndex, which is based on nodes. These nodes are chunks that were parsed by the SimpleNodeParser.\n",
    "\n",
    "By using this setup, the chunks used for retrieval (handled by the RetrieverQueryEngine) are decoupled  from the chunks used for synthesis (handled by the CondenseQuestionChatEngine).  \n",
    "This allows for more efficient and accurate retrieval of relevant documents before retrieving the specific chunks needed for synthesis.\n",
    "\n",
    "https://gpt-index.readthedocs.io/en/latest/end_to_end_tutorials/dev_practices/production_rag.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"./data/test2.txt\"],\n",
    "    encoding=\"utf-8\"\n",
    ").load_data()\n",
    "\n",
    "llm = OpenAI(model=OPENAI_MODEL, temperature=0, max_tokens=512)\n",
    "\n",
    "token_counter = TokenCountingHandler(\n",
    "    tokenizer=tiktoken.encoding_for_model(OPENAI_MODEL).encode\n",
    ")\n",
    "callback_manager = CallbackManager([token_counter])\n",
    "\n",
    "#CHAT_MODE = \"technical\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metadata_extractor = MetadataExtractor(\n",
    "    extractors=[\n",
    "        MarvinMetadataExtractor(\n",
    "            marvin_model=QueryDocument, \n",
    "            llm_model_string=OPENAI_MODEL,\n",
    "            show_progress = True,\n",
    "            callback_manager=callback_manager,\n",
    "\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "logging.info(f\"Number of used tokens: {token_counter.total_embedding_token_count}\")\n",
    "\n",
    "text_splitter = TokenTextSplitter(\n",
    "    separator=\" \", \n",
    "    chunk_size=1024, # 1024 default size\n",
    "    chunk_overlap=128, # default 20\n",
    "    callback_manager=callback_manager\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **SimpleNodeParser** is a tool used in the LlamaIndex library to chunk documents into smaller nodes that can be used for indexing and retrieval purposes. It allows for more efficient processing and retrieval of information from large documents. It takes a list of documents and splits them into nodes of a specific size, with each node inheriting the attributes of the original document, such as metadata, text, and metadata templates.   \n",
    "The chunking is done using a **TokenTextSplitter**, with a default chunk size of 1024 tokens and a chunk overlap of 20 tokens.  \n",
    "The **MetadataExtractor** is used in the LlamaIndex library to extract contextual information from documents and add it as metadata to each node. T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_parser = SimpleNodeParser(\n",
    "    text_splitter=text_splitter,\n",
    "    metadata_extractor=metadata_extractor,\n",
    "    callback_manager = callback_manager,\n",
    ")\n",
    "\n",
    "logging.info(f\"Number of used tokens: {token_counter.total_embedding_token_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "\n",
    "logging.info(f\"Number of used tokens: {token_counter.total_embedding_token_count}\")\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "for node in nodes:\n",
    "    pprint(node.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_category = nodes[0].metadata[\"marvin_metadata\"].get(\"text_category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"\"\"You are a chatbot that responds to all questions about the content of a given document, which is available in the form of embeddings in the given vector database. The user gives you instructions on which questions to answer. \n",
    "    When you write the answers, you need to make sure that the user's expectations are met. Remember that you are an accurate and experienced writer \n",
    "    and you write unique and short answers in the style of a {text_category} text. Don't add anything hallucinatory.\n",
    "    Use friendly, easy-to-read language, and if it is a technical or scientific text, please stay correct and focused.\n",
    "    Responses should be no longer than 10 sentences, unless the user explicitly specifies the number of sentences.\n",
    "\"\"\"\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm, \n",
    "    chunk_size=1024, \n",
    "    chunk_overlap=152,\n",
    "    #system_prompt=system_prompt,\n",
    "    callback_manager=callback_manager,\n",
    ")\n",
    "set_global_service_context(service_context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **VectorStoreIndex** enables efficient indexing and querying of documents based on vector stores. It is a component that allows for the construction and querying of indexes based on vector stores. It is used to store embeddings for input text chunks and provides a query interface for retrieval, querying, deleting, and persisting the index.\n",
    "\n",
    "The VectorStoreIndex can be constructed upon any collection of documents and uses a vector store within the index to store the embeddings. By default, it uses an in-memory SimpleVectorStore that is initialized as part of the default storage context. However, it also supports various other vector stores such as DeepLake, Elasticsearch, Redis, Faiss, Weaviate, Zep, Pinecone, Qdrant, Cassandra, Chroma, Epsilla, Milvus, and Zilliz.\n",
    "\n",
    "Once the index is constructed, you can use it for querying by creating a query engine and executing queries:\n",
    "\n",
    "```\n",
    "# Query index\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index = VectorStoreIndex(nodes, service_context=service_context) # openai api is called with whole text to make the embeddings\n",
    "\n",
    "logging.info(\n",
    "    \"Embedding Tokens: \",\n",
    "    token_counter.total_embedding_token_count,\n",
    "    \"\\n\",\n",
    "    \"LLM Prompt Tokens: \",\n",
    "    token_counter.prompt_llm_token_count,\n",
    "    \"\\n\",\n",
    "    \"LLM Completion Tokens: \",\n",
    "    token_counter.completion_llm_token_count,\n",
    "    \"\\n\",\n",
    "    \"Total LLM Token Count: \",\n",
    "    token_counter.total_llm_token_count,\n",
    "    \"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **VectorIndexRetriever** is used to retrieve nodes from a VectorStoreIndex based on similarity search and therefore allows for efficient retrieval of similar nodes from the index. It takes in a query vector and returns the most similar nodes from the index.\n",
    "\n",
    "Once the VectorIndexRetriever is created, you can use the retrieve() method to perform a similarity search. You pass in the query vector and it returns the most similar nodes from the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_retriever = VectorIndexRetriever(\n",
    "    index=vector_index,\n",
    "    similarity_top=2,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **RetrieverQueryEngine** is an end-to-end pipeline that allows you to perform queries and retrieve relevant context from a knowledge base using a retriever. It takes in a natural language query and returns a response along with the reference context retrieved from the knowledge base.\n",
    "\n",
    "The RetrieverQueryEngine uses a retriever, which defines how to efficiently retrieve relevant context from a knowledge base when given a query. One example of a retriever is the VectorIndexRetriever, which retrieves nodes from a VectorStoreIndex based on similarity search.\n",
    "\n",
    "The RetrieverQueryEngine handles the orchestration of the retrieval process and provides a convenient interface for querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vector_query_engine = RetrieverQueryEngine(\n",
    "    retriever=vector_retriever,\n",
    "    response_synthesizer=get_response_synthesizer(),\n",
    "    callback_manager=callback_manager,\n",
    ")\n",
    "logging.info(f\"Number of used tokens: {token_counter.total_embedding_token_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **CondenseQuestionChatEngine** is designed to condense a set of similar questions into a single representative question. It is useful in scenarios where there are multiple variations of the same question and you want to consolidate them for more efficient querying and retrieval.\n",
    "\n",
    "The use case for the CondenseQuestionChatEngine is to improve the performance and accuracy of question-answering systems by reducing redundancy and optimizing the retrieval process. By condensing similar questions, it helps to eliminate duplicate queries and improve the overall user experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.chat_engine.condense_question import CondenseQuestionChatEngine\n",
    "\n",
    "# list of `ChatMessage` objects\n",
    "# custom_chat_history = [\n",
    "#     ChatMessage(\n",
    "#         role=MessageRole.USER, \n",
    "#         content='Hello assistant, we are having a insightful discussion about the given text content.'\n",
    "#     ), \n",
    "#     ChatMessage(\n",
    "#         role=MessageRole.ASSISTANT, \n",
    "#         content='Okay, sounds good.'\n",
    "#     )\n",
    "# ]\n",
    "chat_engine = CondenseQuestionChatEngine.from_defaults(\n",
    "    query_engine=vector_query_engine, \n",
    "    #condense_question_prompt=custom_prompt,\n",
    "    #chat_history=custom_chat_history,\n",
    "    \n",
    "    verbose=True,\n",
    "    callback_manager=callback_manager,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat_engine.chat(\"What did Einstein do?\")\n",
    "logging.info(f\"Number of used tokens: {token_counter.total_embedding_token_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.source_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat_engine.chat(\"What was Einsteins' favorite food?\")\n",
    "logging.info(f\"Number of used tokens: {token_counter.total_embedding_token_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://github.com/jerryjliu/llama_index/blob/main/docs/examples/index_structs/doc_summary/DocSummary.ipynb\n",
    "- https://betterprogramming.pub/llamaindex-0-6-0-a-new-query-interface-over-your-data-331996d47e89\n",
    "- https://gpt-index.readthedocs.io/en/latest/examples/query_engine/CustomRetrievers.html\n",
    "- https://gpt-index.readthedocs.io/en/latest/core_modules/query_modules/chat_engines/usage_pattern.html\n",
    "\n",
    "- https://gpt-index.readthedocs.io/en/latest/examples/metadata_extraction/MarvinMetadataExtractorDemo.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
