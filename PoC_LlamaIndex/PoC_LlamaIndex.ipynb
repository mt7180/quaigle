{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import (\n",
    "    VectorStoreIndex, \n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    ServiceContext,\n",
    "    load_index_from_storage,\n",
    "    set_global_service_context,\n",
    ")\n",
    "\n",
    "from llama_index.callbacks import CallbackManager, TokenCountingHandler\n",
    "from llama_index.llms import OpenAI\n",
    "from openai import log as openai_log\n",
    "import tiktoken\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "openai_log = \"debug\"\n",
    "\n",
    "load_dotenv()\n",
    "# load_dotenv(\"../.env\")\n",
    "API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "OPENAI_MODEL = \"gpt-3.5-turbo\"\n",
    "CHAT_MODE = \"technical\"\n",
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"./data/test2.txt\"],\n",
    "    encoding=\"utf-8\"\n",
    ").load_data()\n",
    "\n",
    "llm = OpenAI(model=OPENAI_MODEL, temperature=0, max_tokens=256)\n",
    "\n",
    "token_counter = TokenCountingHandler(\n",
    "    tokenizer=tiktoken.encoding_for_model(OPENAI_MODEL).encode\n",
    ")\n",
    "callback_manager = CallbackManager([token_counter])\n",
    "\n",
    "system_prompt = f\"\"\"You are a chatbot answering to to all questions concerning the content of a given\n",
    "    text file. The user will give you instructions on what questions to answer. \n",
    "    When you write the answers, you will need to ensure that the\n",
    "    user's expectations are met. Remember, you are an accurate and experianced author \n",
    "    and you write unique and short answers. Keep your answers {CHAT_MODE} and based on facts â€“ do not hallucinate features..\n",
    "    You should use friendly, easy to read language, but stay correct and focussed.\n",
    "    The answers should not have more than 10 sentences.\n",
    "\"\"\"\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm, \n",
    "    chunk_size=800, \n",
    "    chunk_overlap=20,\n",
    "    system_prompt=system_prompt,\n",
    "    callback_manager=callback_manager,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "# index.storage_context.persist(vector_store_fname=\"vectorstore\")\n",
    "logging.info(f\"Number of used tokens: {token_counter.total_embedding_token_count}\")\n",
    "index.storage_context.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_engine = index.as_query_engine()\n",
    "# response = query_engine.query(\"Please summarize the text\")\n",
    "# logging.info(f\"Number of used tokens: {token_counter.total_embedding_token_count}\")\n",
    "# display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = load_index_from_storage(storage_context=storage_context)\n",
    "logging.info(f\"Number of used tokens: {token_counter.total_embedding_token_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index.as_query_engine(response_mode = \"tree_summarize\")\n",
    "index.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.memory import ChatMemoryBuffer\n",
    "memory = ChatMemoryBuffer.from_defaults(token_limit=1500)\n",
    "chat_engine = index.as_chat_engine(\n",
    "    chat_mode=\"context\",\n",
    "    memory=memory,\n",
    "    service_context = service_context\n",
    ")\n",
    "response = chat_engine.chat(\"Please summarize the text\")\n",
    "logging.info(f\"Number of used tokens: {token_counter.total_embedding_token_count}\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "token_counter.reset_counts()\n",
    "resp = chat_engine.chat(\"Please make a list of the key facts given in the text\")\n",
    "\n",
    "logging.info(\n",
    "    \"Embedding Tokens: \",\n",
    "    token_counter.total_embedding_token_count,\n",
    "    \"\\n\",\n",
    "    \"LLM Prompt Tokens: \",\n",
    "    token_counter.prompt_llm_token_count,\n",
    "    \"\\n\",\n",
    "    \"LLM Completion Tokens: \",\n",
    "    token_counter.completion_llm_token_count,\n",
    "    \"\\n\",\n",
    "    \"Total LLM Token Count: \",\n",
    "    token_counter.total_llm_token_count,\n",
    "    \"\\n\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### streaming the response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat_engine = index.as_chat_engine()\n",
    "# streaming_response = chat_engine.stream_chat(\"Tell me a joke.\")\n",
    "# for token in streaming_response.response_gen:\n",
    "#     print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = SimpleWebPageReader(html_to_text=True).load_data([\"https://en.wikipedia.org/wiki/South_Africa\"])\n",
    "chat_engine = index.as_chat_engine(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_engine = index.as_chat_engine(chat_mode='react', verbose=True)\n",
    "response = chat_engine.chat('Use the tool to answer: What happened in the year 1652?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ai_fn\n",
    "def classify_text(text: str) -> Literal['sports', 'politics', 'technology']:\n",
    "    '''\n",
    "        Correctly classifies the passed `text` into one of the predefined categories. \n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some useful Links:\n",
    "- https://gpt-index.readthedocs.io/en/latest/examples/metadata_extraction/MarvinMetadataExtractorDemo.html\n",
    "- https://gpt-index.readthedocs.io/en/stable/examples/chat_engine/chat_engine_context.html?ref=blog.streamlit.io\n",
    "- https://gpt-index.readthedocs.io/en/latest/core_modules/query_modules/chat_engines/root.html\n",
    "- https://gpt-index.readthedocs.io/en/latest/examples/callbacks/TokenCountingHandler.html\n",
    "- https://colab.research.google.com/drive/1F-4r976AhCYmH9lK89S4t_dEAqcqho0S#scrollTo=bwdh1lj6g_pz\n",
    "\n",
    "- sql: https://gpt-index.readthedocs.io/en/latest/examples/index_structs/struct_indices/SQLIndexDemo.html\n",
    "\n",
    "-retrievers: https://github.com/SamurAIGPT/LlamaIndex-course/blob/main/fundamentals/Fundamentals.ipynb\n",
    "\n",
    "-chat engine from query engine: https://gpt-index.readthedocs.io/en/latest/core_modules/query_modules/chat_engines/usage_pattern.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
