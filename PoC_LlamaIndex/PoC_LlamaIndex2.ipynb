{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat engine in Condense Question mode with explicitely specified vector_retriever and response_synthesizer\n",
    "### trying to integrate marvins ai_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from marvin import ai_model\n",
    "from llama_index.bridge.pydantic import BaseModel, Field\n",
    "from llama_index.callbacks import CallbackManager, TokenCountingHandler\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.llms import ChatMessage, MessageRole\n",
    "from llama_index import (\n",
    "    VectorStoreIndex, \n",
    "    SimpleDirectoryReader,\n",
    "    ServiceContext,\n",
    "    set_global_service_context,\n",
    "    get_response_synthesizer,\n",
    ")\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index.node_parser.extractors import (\n",
    "    MetadataExtractor,\n",
    ")\n",
    "from llama_index.text_splitter import TokenTextSplitter\n",
    "from llama_index.node_parser.extractors.marvin_metadata_extractor import (\n",
    "    MarvinMetadataExtractor,\n",
    ")\n",
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "\n",
    "from openai import log as openai_log\n",
    "import tiktoken\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import certifi\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "openai_log = \"debug\"\n",
    "\n",
    "load_dotenv()\n",
    "# load_dotenv(\"../.env\")\n",
    "# workaround for mac to solve \"SSL: CERTIFICATE_VERIFY_FAILED Error\"\n",
    "\n",
    "\n",
    "os.environ[\"REQUESTS_CA_BUNDLE\"] = certifi.where()\n",
    "os.environ[\"SSL_CERT_FILE\"] = certifi.where()\n",
    "\n",
    "API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "OPENAI_MODEL = \"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORY_LABELS = [\n",
    "    \"Technical\",\n",
    "    \"Science-Fiction\",\n",
    "    \"Poetry\",\n",
    "    \"Fantasy\",\n",
    "    \"Mystery\",\n",
    "    \"Romance\",\n",
    "    \"Historical\",\n",
    "    \"Fiction\",\n",
    "    \"Self-Help\",\n",
    "    \"Biography\",\n",
    "    \"Travelogue\",\n",
    "    \"Horror\",\n",
    "    \"Comedy\",\n",
    "    \"Thriller\",\n",
    "    \"Science\",\n",
    "    \"Philosophy\",\n",
    "    \"Memoir\",\n",
    "    \"Cookbook\",\n",
    "    \"Business\",\n",
    "    \"Drama\",\n",
    "    \"Satire\",\n",
    "]\n",
    "\n",
    "@ai_model\n",
    "class QueryDocument(BaseModel):\n",
    "    #name: str = Field(..., description=\"The name of the document\")\n",
    "    description: str = Field(..., description=\"a brief summary of the document content.\")\n",
    "    text_category: str = Field(...,description=f\"best matching text category from the following list: {str(CATEGORY_LABELS)}\")\n",
    "    \n",
    "    # def __init__(self, name):\n",
    "    #     super().__init__()\n",
    "    #     self.name = name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"./data/test2.txt\"],\n",
    "    encoding=\"utf-8\"\n",
    ").load_data()\n",
    "\n",
    "llm = OpenAI(model=OPENAI_MODEL, temperature=0, max_tokens=512)\n",
    "\n",
    "token_counter = TokenCountingHandler(\n",
    "    tokenizer=tiktoken.encoding_for_model(OPENAI_MODEL).encode\n",
    ")\n",
    "callback_manager = CallbackManager([token_counter])\n",
    "\n",
    "#CHAT_MODE = \"technical\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "metadata_extractor = MetadataExtractor(\n",
    "    extractors=[\n",
    "        MarvinMetadataExtractor(\n",
    "            marvin_model=QueryDocument, \n",
    "            llm_model_string=OPENAI_MODEL,\n",
    "            show_progress = True,\n",
    "            callback_manager=callback_manager,\n",
    "\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "logging.info(f\"Number of used tokens: {token_counter.total_embedding_token_count}\")\n",
    "\n",
    "text_splitter = TokenTextSplitter(\n",
    "    separator=\" \", \n",
    "    chunk_size=1024, \n",
    "    chunk_overlap=128,\n",
    "    callback_manager=callback_manager\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "node_parser = SimpleNodeParser(\n",
    "    text_splitter=text_splitter,\n",
    "    metadata_extractor=metadata_extractor,\n",
    "    callback_manager = callback_manager,\n",
    ")\n",
    "\n",
    "logging.info(f\"Number of used tokens: {token_counter.total_embedding_token_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "\n",
    "logging.info(f\"Number of used tokens: {token_counter.total_embedding_token_count}\")\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "for node in nodes:\n",
    "    pprint(node.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_category = nodes[0].metadata[\"marvin_metadata\"].get(\"text_category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"\"\"You are a chatbot that responds to all questions about the content of a given document, which is available in the form of embeddings in the given vector database. The user gives you instructions on which questions to answer. \n",
    "    When you write the answers, you need to make sure that the user's expectations are met. Remember that you are an accurate and experienced writer \n",
    "    and you write unique and short answers in the style of a {text_category} text. Don't add anything hallucinatory.\n",
    "    Use friendly, easy-to-read language, and if it is a technical or scientific text, please stay correct and focused.\n",
    "    Responses should be no longer than 10 sentences, unless the user explicitly specifies the number of sentences.\n",
    "\"\"\"\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm, \n",
    "    chunk_size=1024, \n",
    "    chunk_overlap=152,\n",
    "    #system_prompt=system_prompt,\n",
    "    callback_manager=callback_manager,\n",
    ")\n",
    "set_global_service_context(service_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index = VectorStoreIndex(nodes, service_context=service_context) # openai api is called with whole text to make the embeddings\n",
    "logging.info(\n",
    "    \"Embedding Tokens: \",\n",
    "    token_counter.total_embedding_token_count,\n",
    "    \"\\n\",\n",
    "    \"LLM Prompt Tokens: \",\n",
    "    token_counter.prompt_llm_token_count,\n",
    "    \"\\n\",\n",
    "    \"LLM Completion Tokens: \",\n",
    "    token_counter.completion_llm_token_count,\n",
    "    \"\\n\",\n",
    "    \"Total LLM Token Count: \",\n",
    "    token_counter.total_llm_token_count,\n",
    "    \"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_retriever = VectorIndexRetriever(\n",
    "    index=vector_index,\n",
    "    similarity_top=2,\n",
    "\n",
    ")\n",
    "\n",
    "vector_query_engine = RetrieverQueryEngine(\n",
    "    retriever=vector_retriever,\n",
    "    response_synthesizer=get_response_synthesizer(),\n",
    "    callback_manager=callback_manager,\n",
    ")\n",
    "logging.info(f\"Number of used tokens: {token_counter.total_embedding_token_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.chat_engine.condense_question import CondenseQuestionChatEngine\n",
    "\n",
    "# list of `ChatMessage` objects\n",
    "# custom_chat_history = [\n",
    "#     ChatMessage(\n",
    "#         role=MessageRole.USER, \n",
    "#         content='Hello assistant, we are having a insightful discussion about the given text content.'\n",
    "#     ), \n",
    "#     ChatMessage(\n",
    "#         role=MessageRole.ASSISTANT, \n",
    "#         content='Okay, sounds good.'\n",
    "#     )\n",
    "# ]\n",
    "chat_engine = CondenseQuestionChatEngine.from_defaults(\n",
    "    query_engine=vector_query_engine, \n",
    "    #condense_question_prompt=custom_prompt,\n",
    "    #chat_history=custom_chat_history,\n",
    "    \n",
    "    verbose=True,\n",
    "    callback_manager=callback_manager,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat_engine.chat(\"What did Einstein do?\")\n",
    "logging.info(f\"Number of used tokens: {token_counter.total_embedding_token_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.source_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat_engine.chat(\"What was Einsteins' favorite food?\")\n",
    "logging.info(f\"Number of used tokens: {token_counter.total_embedding_token_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://github.com/jerryjliu/llama_index/blob/main/docs/examples/index_structs/doc_summary/DocSummary.ipynb\n",
    "- https://betterprogramming.pub/llamaindex-0-6-0-a-new-query-interface-over-your-data-331996d47e89\n",
    "- https://gpt-index.readthedocs.io/en/latest/examples/query_engine/CustomRetrievers.html\n",
    "- https://gpt-index.readthedocs.io/en/latest/core_modules/query_modules/chat_engines/usage_pattern.html\n",
    "\n",
    "- https://gpt-index.readthedocs.io/en/latest/examples/metadata_extraction/MarvinMetadataExtractorDemo.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
